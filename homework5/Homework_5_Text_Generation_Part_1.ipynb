{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljhQ3WC79pAa"
   },
   "source": [
    "# From: Transformers and Text-Generation\n",
    "by Liam Dugan (UPenn). \n",
    "\n",
    "\n",
    "Please write your answers and code in the cells with questions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwQJB7Ft9wJ3"
   },
   "source": [
    "----------\n",
    "\n",
    "For this homework, we will take ideas from the entire class: language models, text generation, vector-based word representations, syntactic analysis, and neural networks. We'll be using large, pre-trained language models to generate text, and studying how we can fine-tune these large language models to generate text in whatever genre and style we want!\n",
    "\n",
    "In this assignment you will get:\n",
    "1. An overview of the \"Transformer\" architecture is and why it is particularly well suited for Natural Language Processing tasks\n",
    "2. An introduction to the Generative Pretrained Transformer (GPT) family, which is a set of large-scale language models that can be used to generate text that often sounds like it was written by a human.\n",
    "3. Experience with using the HuggingFace package to fine-tune these models to generate text that sounds like it comes from a specific source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLt2-8LK9pO_"
   },
   "source": [
    "# Part 1: What is a Transformer? (Reading)\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://media.giphy.com/media/VeWllmR9zfaco/giphy.gif\" />\n",
    "<figcaption>(It's probably not this guy, right?)</figcaption>\n",
    "</figure>\n",
    "\n",
    "### The Transformer\n",
    "\n",
    "The current state-of-the-art for a variety of natural language processing tasks belongs to the **Transformer** architecture, first published December 6th 2017. \n",
    "\n",
    "The Transformer can be thought of as a big feed-forward network with every feed-forward layer containing something called an \"attention module\". \n",
    "\n",
    ">You might be wondering: why are we moving back to feed-forward networks after having so much success with recurrent neural networks and variants like LSTMs? Aren't RNNs naturally poised to handle sequences as their inputs? Well, as it turns out, the sequential nature of RNNs make them really difficult to train in a distributed/parallel fashion. So while RNNs make more sense to use on sequences of inputs, serial networks such as the transformer can be trained much faster, allowing orders of magnitude more training data to be used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kDBzHuT-vPj"
   },
   "source": [
    "\n",
    "### Reading \\# 1 - What is a Transformer?\n",
    "\n",
    "In order to get a good grasp on exactly *why* these models are so good it's important to understand what they are and how they work. \n",
    "\n",
    "Your first task for this homework is to read the blog post [\"The Illustrated Transformer\" by Jay Alammar](http://jalammar.github.io/illustrated-transformer/). This blog post explains the transformer architecture (and the all-important \"Attention Module\") with helpful visualizations and diagrams. \n",
    "\n",
    "**You should read this post very closely and understand exactly what the Transformer is and how it works. Once you're finished reading, answer the following questions in 2-3 sentences each.**\n",
    "\n",
    "1. (2 pts) What is Self-Attention (at a high level)?\n",
    "\n",
    "   > As the model processes each word (each position in the input sequence), self-attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. It is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n",
    "\n",
    "2. (2 pts) How is Self-Attention computed?\n",
    "\n",
    "   > $$Atten = \\frac {softmax(Q*K.T)} {\\sqrt{d_k}} * V$$\n",
    "\n",
    "3. (2 pts) What do the \"Query\", \"Key\", and \"Value\" vectors encode (at a high level)?\n",
    "\n",
    "   > When we compute Self-Attention, we need to score each word of the input sentence against this word. The score is calculated by taking the dot product of the \"Query\" vector with the \"Key\" vector of the respective word we’re scoring. Then, we will multiply each \"Value\" vector by the softmax score. It means keep intact the values of the words we want to focus on, and drown-out irrelevant words.\n",
    "\n",
    "4. (2 pts) What is an attention \"head\" and why should we use multiple heads?\n",
    "\n",
    "   > An attention \"head\" means we do self-attention once, muliple heads means we do self-attention multiple times. Using multiple heads expands the model’s ability to focus on different positions and gives the attention layer multiple “representation subspaces”.\n",
    "\n",
    "5. (2 pts) What are positional embeddings?\n",
    "\n",
    "   > Positional embeddings represent the order of the words in the input sequence. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence\n",
    "\n",
    "6. (2 pts) Why are positional embeddings important?\n",
    "\n",
    "   > Because the self-attention do not have information about the order of words, but the order of words is important, such as \"I love you\" and \"you love me\" are total different.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ-OTakPCPJc"
   },
   "source": [
    "-----------\n",
    "### Reading \\# 2 - Transformer Language Models\n",
    "\n",
    "On June 11th 2018, OpenAI released a model named **GPT**, standing for *Generative Pre-Trained Transformer*. This model was a Transformer architecture that was modified such that it could be used for Text Generation instead of sequence to sequence modeling. This model was also pre-trained, which means that anyone could download the fully trained model and use it without needing to train the model themselves. \n",
    "\n",
    "On February 14th 2019, OpenAI released a blog post detailing a brand new version of GPT that had an insane **1.5 billion parameters**. They named this version **GPT-2**. To train such a large model, OpenAI crawled 40GB worth of text from the web (roughly 20,000,000,000 words). \n",
    "\n",
    "GPT-2 is an extremely impressive language model that can generate text that often sounds so plausible that it seems like it might have been written by a human.  Here is an example of what GPT-2 can generate, taken from [OpenAI’s blog post](https://openai.com/blog/better-language-models/):\n",
    "\n",
    "**Human-Written Prompt:**\n",
    "> “Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.”\n",
    "\n",
    "**Model Continuation:**\n",
    "> The 19-year-old singer was caught on camera being escorted out of the store by security guards.\n",
    "> \n",
    "> The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back.\n",
    "> \n",
    "> Scroll down for video\n",
    "> \n",
    "> Shoplifting: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today (pictured)\n",
    "> \n",
    "> The singer was also wearing a pair of black-rimmed glasses, a black jacket, black jeans and black sandals.\n",
    "> \n",
    "> She was carrying a pair of black and white striped gloves and a small black bag.\n",
    "\n",
    "*(To be clear: Miley Cyrus was never actually arrested for shoplifting)*.  \n",
    "\n",
    "The quality of the model's output was so good, that the researchers at OpenAI were worried that, in addition to positive applications, it might be used for malicious purposes like generating fake news, impersonating people online, creating abusive or fake content to post on social media, or automating spam and phishing scams. In a [demo to Wired Magazine](https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/), the reporter typed in “Hillary Clinton and George Soros” and GPT-2 generated a crazy rant with lots of conspiracy theories: \n",
    "> Hillary Clinton and George Soros are a perfect match, and their agenda appears to be to create a political movement where Soros and his political machine and Clinton are two of the only major players. This is the first time Soros and Clinton have been caught on tape directly colluding in promoting the same false narrative. One of the key revelations in the leaked audio was Clinton's admission to a Russian banker that she knew about the Uranium One deal before it was approved by Congress. Clinton was shown sharing the same talking points that were originally drafted by a Fusion GPS contractor hired by an anti-Trump Republican donor. The leaked audio is the clearest evidence yet that the Clinton campaign and the Hillary Foundation colluded with Fusion GPS to manufacture propaganda against President Trump.\n",
    "\n",
    "\n",
    "They were concerned enough that they labeled GPT-2 \"too dangerous to release\", and OpenAI initially refused to release their dataset, training code, or GPT-2 model weights.  OpenAI decided to release in a delayed, phased fashion so that researchers could spend time working on automatic detection of generated text.\n",
    "\n",
    "In this homework, you'll get to be the judge of how good GPT-2 is, as you'll be using it yourself to generate text!\n",
    "\n",
    "**To start your journey into the world of Text Generation, you should read Part 1 of the blog post [\"The Illustrated GPT-2\" by Jay Alammar](http://jalammar.github.io/illustrated-gpt2/) and answer the following questions in 2-3 sentences each**\n",
    "\n",
    "1. (4 pts) How does the architecture of GPT-2 differ from the standard Encoder-Decoder Transformer model?\n",
    "   > GPT-2 consists of solely stacked decoder blocks from the transformer architecture. Instead of using encoder to model sequences and use decoder to generate words, GPT-2 only use decoder to model and generating. In GPT-2 masked self-attention is used : the decoder is only allowed to glean information from the prior words in the sentence (plus the word itself).\n",
    "2. (4 pts) What is the difference between \"Masked Self-Attention\" and \"Self-Attention\"\n",
    "   > The masked self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "3. (4 pts) What are logits? How are they computed? and How does GPT-2 use them to decide which word to predict next?\n",
    "   > Logits are probability distribution over all possible integer token IDs. Logits are the output of GPT-2, and then use softmax, $\\frac {exp(X_i)} {sum(exp(X_i)}$, to compute probability. And predict next word with high probability.\n",
    "\n",
    "### Aside: GPT-3 \n",
    "\n",
    "On June 11th 2020, OpenAI released GPT-3 [(paper)](https://arxiv.org/pdf/2005.14165.pdf) [(wikipedia)](https://en.wikipedia.org/wiki/GPT-3). This model has an unfathomable **175 billion parameters** (100x larger than GPT-2!) and was trained on 570GB of text! This model is virtually indistinguishable from human output and can generate text about any topic and in any style with only a few words of priming text. It is by far the largest language model ever trained and it can do some very terrifying things.\n",
    "\n",
    "GPT-3 Can:\n",
    "- Generate JSX code off natural language descriptions\n",
    "- Generate Emojis based off of descriptions of the feeling\n",
    "- Generate regular expressions off natural language descriptions\n",
    "- Generate website mockups off natural language descriptions\n",
    "- Generate charts with titles, labels and legends from natural language descriptions\n",
    "- Explain python code in plain english\n",
    "- Automatically generate quiz questions (and grade them)\n",
    "- Generate Latex from natural language descriptions\n",
    "- Generate Linux commands from natural language descriptions\n",
    "- Generate a Machine Learning model from natural language descriptions\n",
    "\n",
    "[Here's a collection of 21 things GPT-3 can do (with examples)](https://machinelearningknowledge.ai/openai-gpt-3-demos-to-convince-you-that-ai-threat-is-real-or-is-it/#OpenAI_GPT-3_Demos)\n",
    "\n",
    "[Here's a NYT article about how GPT-3 can write code, poetry, and argue](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html)\n",
    "\n",
    "[Here's an article GPT-3 wrote for The Guardian about how it loves humans and would never subjugate humanity](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)\n",
    "\n",
    "**You may optionally choose to read Jay Alammar's most recent blog post [\"How GPT3 Works - Visualizations and Animations\"](http://jalammar.github.io/how-gpt3-works-visualizations-animations/) from July 2020 if you're curious as to how GPT-3 differs from GPT-2**\n",
    "\n",
    "Similarly to GPT-2, OpenAI has decided not to release GPT-3, this time opting to put GPT-3 behind an API which you need to request permission to use. This allows them to control exactly who can generate text and what type of text is generated. While this is a good solution in the short term, the long term implications of GPT-3 are still unclear.\n",
    "\n",
    "If you are interested in trying out GPT-3 yourself, feel free to [Join the OpenAI API Waitlist](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DFqk7jra-Li"
   },
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y14gbpSjOTt7"
   },
   "source": [
    "# Part 2: GPT-2 Text Generation with HuggingFace\n",
    "\n",
    "Phew, that was a lot of reading. Now lets get to the fun part! Let's use the transformer to generate some text!!\n",
    "\n",
    "We will use the [Transformers library from HuggingFace](https://transformer.huggingface.co), which provides support for many Transformer-based language models like GPT-2. \n",
    "\n",
    "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in `Runtime > Change runtime type` before running this Colab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6yWI0ae9knK",
    "outputId": "2987bb61-6ff0-4b9d-e305-89e528941259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3J0IKG24rkH",
    "outputId": "8b52beb0-de7e-4a65-95fd-a77145000d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 11 04:09:00 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CpMoXtZm5jC5"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import transformers\n",
    "logging.getLogger(\"transformers.generation_utils\").setLevel(logging.ERROR)  # No warning on sample size\n",
    "logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLdW3FOcIRqi"
   },
   "source": [
    "## 2.1 The 'Pipeline' Interface\n",
    "\n",
    "The simplest way to use the HuggingFace library is to use their [Pipeline interface](https://huggingface.co/transformers/main_classes/pipelines.html)\n",
    "\n",
    "There are many different types of Pipelines available but in this section we'll use the TextGenerationPipeline to get up and running with pretrained gpt2 as fast as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p1IEBxEl5PlA"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-MCgVfPSG8Sg"
   },
   "outputs": [],
   "source": [
    "# Note: device=0 means to use GPU, device=-1 is to use CPU\n",
    "generator = pipeline('text-generation', model='gpt2', device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_gc4VRmHCHA",
    "outputId": "c45e8694-b753-45d6-de33-5a18e74e1c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love cats. I am extremely allergic to them. I am a vet, and since I love cats, I've spent the last few days just making up for all the other things I have done for cats.\n",
      "\n",
      "But now, instead of\n"
     ]
    }
   ],
   "source": [
    "outputs = generator('I love cats.')\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWDdGag_UrkA"
   },
   "source": [
    "Note that the 'text-generation' pipeline will work with any **auto-regressive** language model (a.k.a 'causal-lm' models according to the HuggingFace lingo). You can find a list of all such models here https://huggingface.co/models?filter=causal-lm. \n",
    "\n",
    "(6 pts) **Your first task is to use the Pipeline interface to get generation output below for at least two different 'causal-lm' models (One of these two can be a different version of GPT2, but make sure at least one is a non-gpt family language model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vYbWdguUU2Bl"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE FOR MODEL 1\n",
    "generator_1 = pipeline('text-generation', model='xlnet-base-cased', device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSFdQh5TWIM-",
    "outputId": "07a099b1-869f-4656-abf2-f0e036eb3a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love cats. I’m not one for people giving them away, but I’m the kind who’ll give them to someone who wants to adopt them. I want to live a life of sustaining a family (without their children) for a long period of time after they grow old and their children are allowed to leave their parents), but I really do want to be a good mother to someone who\n"
     ]
    }
   ],
   "source": [
    "outputs = generator_1('I love cats.')\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lZLHYzr4U54s"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE FOR MODEL 2\n",
    "generator_2 = pipeline('text-generation', model='microsoft/DialoGPT-medium', device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wRdylZQWoEO",
    "outputId": "63f53e08-f67f-43eb-96a3-056cd32930c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love cats.They are so cute .\n"
     ]
    }
   ],
   "source": [
    "outputs = generator_2('I love cats.')\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7T-UKAsRYp1p"
   },
   "outputs": [],
   "source": [
    "# nlp = pipeline(\"question-answering\")\n",
    "\n",
    "# context = \"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the `run_squad.py`.\"\n",
    "\n",
    "# print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "# print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joicqRgHJHcW"
   },
   "source": [
    "## 2.2 Dissecting the Pipeline\n",
    "Now that was easy!\n",
    "\n",
    "As beautiful and easy as the Pipeline interface is, we want to know what's going on under the hood!\n",
    "\n",
    "There are four main steps to a text generation pipeline:\n",
    "1. (Tokenize) Turn the raw input text into a vector of integer token IDs using a tokenizer\n",
    "\n",
    "2. (Encode) Feed those token IDs into the language model by querying for each token's embedding in the model's embedding matrix (the \"encoder\") and then feed the \"encoded\" sequence into the decoder module\n",
    "\n",
    "3. (Decode) The decoder will output logits (a probability distribution over all possible integer token IDs) and we sample from those logits to get our next token -- repeat until EOS token is generated or we hit max_length\n",
    "\n",
    "4. (Detokenize) Take the output sequence of token IDs and turn them from integer token IDs back to tokens with the tokenizer\n",
    "\n",
    "Below you'll see how HuggingFace does this:\n",
    "\n",
    "First we have to initialize both the tokenizer and the model from their pre-trained checkpoints. Note that the tokenizer has to match the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nRgGzAuTJakH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azhDlBgYM9bP",
    "outputId": "8bc5aea5-d79c-4cad-cc51-b9b17e866a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token IDs: tensor([[15496,    11,   703,   389,   345,    30]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### Step 1: Tokenize the input into integer token IDs\n",
    "inputs = tokenizer.encode(\"Hello, how are you?\", return_tensors='pt').to(model.device)\n",
    "print(\"Input Token IDs: \" + str(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaFR6j-5Rz6C",
    "outputId": "edd6c6cc-6ef0-42cc-dc24-de7c8e84e3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Token IDs: tensor([[15496,    11,   703,   389,   345,    30,   198,   198,    40,  1101,\n",
      "           257,  1310,  1643,   286,   257, 34712,    13,   314,  1101,   257]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### Step 2 and 3: Feed in the integer token IDs and get out a sequence of token IDs as output\n",
    "outputs = model.generate(inputs)\n",
    "print(\"Output Token IDs: \" + str(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tO1NWK9DSD8u",
    "outputId": "10eee76d-7833-4961-da35-c24264eafc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: [\"Hello, how are you?\\n\\nI'm a little bit of a nerd. I'm a\"]\n"
     ]
    }
   ],
   "source": [
    "#### Step 4: Feed in the integer token IDs and get out a sequence of token IDs as output\n",
    "output_text = [tokenizer.decode(x) for x in outputs]\n",
    "print(\"Output Text: \" + str(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNRN-CfvOiU0"
   },
   "source": [
    "Now that you have dissected the pipeline, it's time to play with some common parameters!\n",
    "\n",
    "[Check out this demo notebook from HuggingFace](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) for a good overview of the different generation parameters and what they do (with example code!).\n",
    "\n",
    "The full documentation on all of the parameters you can use in the generate function can be found [here](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)\n",
    "\n",
    "As an example, below we have a call to generate that:\n",
    "- randomly samples from the top 50 words in the output distribution (rather than just greedily picking the best one every time)\n",
    "- downweights the probability of all previously generated tokens by a factor of 1.2 (to prevent repetition)\n",
    "- goes on for 512 tokens, because its more interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fNJ5InvTtgm",
    "outputId": "6144f093-606d-470f-bd19-b3544cbc5c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Los Angeles Lakers, who have long taken pride in playing for the \"Unsung Hero\" of their sport (and are currently being praised by many players around town), will be looking to play a very interesting game on Saturday. - ESPN's Chris Cillizza covers college basketball and its importance with three hours per day starting at 3pm Pacific Time from San Francisco State University where he shares how his coaches approach these games every week or so along their season ticket list that comes each weekday afternoon:\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"The Los Angeles Lakers\", return_tensors='pt').to(model.device)\n",
    "outputs = model.generate(\n",
    "      inputs,\n",
    "      do_sample=True,          # Randomly sample from the logits instead of greedily picking next word with highest probability\n",
    "      top_k=50,                 # Only sample from the top 50 most likely words\n",
    "      repetition_penalty=1.2,    # Downweights the probability of all previously generated tokens by a factor of 1.2\n",
    "      max_length=512          # Generate for a maximum of 512 tokens\n",
    "  )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace('\\n',' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSb6SAgnV4GD"
   },
   "source": [
    "**Your job is to provide two different examples of generation output from GPT-2 with different choices of generation parameters. You must also provide a 1-2 sentence explanation of what these parameters do and how they affect your output**\n",
    "\n",
    "Feel free to get creative with this! Really poke around and try to find the combination of settings that gives you the best sounding text! The ways in which these parameters affect how 'human-like' a section of generated text sounds is an area of active research. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-4Xj-9mXNNF"
   },
   "source": [
    "(4 pts) YOUR ANSWER HERE - EXPLANATION FOR HPARAM VARIATION 1\n",
    "> Using beam search, with 10 beams, controling repeat ngram size less than 2, and use early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhfhpC_qiziO",
    "outputId": "f32293e5-11ea-4626-9c7f-d1690386dec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: The Los Angeles Lakers are the only team in the NBA that has won at least 100 games in each of the past three seasons.  The Lakers have won three of their past four games at Staples Center, including a 108-98 victory over the \n",
      "1: The Los Angeles Lakers are the only team in the NBA that has won at least 100 games in each of the past three seasons.  The Lakers have won three of their past four games at Staples Center, including a 103-98 victory over the \n",
      "2: The Los Angeles Lakers are the only team in the NBA that has won at least 100 games in each of the past three seasons.  The Lakers have won three of their past four games at Staples Center, including a 103-99 victory over the \n",
      "3: The Los Angeles Lakers are the only team in the NBA that has won at least 100 games in each of the past three seasons.  The Lakers have won three of their past four games at Staples Center, including a 103-100 victory over the \n",
      "4: The Los Angeles Lakers are the only team in the NBA that has won at least 100 games in each of the past three seasons.  The Lakers have won three of their past four games at Staples Center, including a 103-98 win over the \n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"The Los Angeles Lakers\", return_tensors='pt').to(model.device)\n",
    "outputs = model.generate(\n",
    "      inputs,\n",
    "      max_length=50, \n",
    "      num_beams=10, \n",
    "      no_repeat_ngram_size=2,\n",
    "      num_return_sequences=5, \n",
    "      early_stopping=True\n",
    "  )\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(outputs):\n",
    "    print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)).replace('\\n',' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8G2LIiIWX5R"
   },
   "source": [
    "(4 pts) YOUR ANSWER HERE -- EXPLANATION FOR HPARAM VARIATION 2\n",
    "> Use top_p sample with top_k sample, and use temperature=1, reutrn 3 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5i14ikmiyZ1",
    "outputId": "7a514930-fd4f-46e4-bf5a-0f9e3613072b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: The Los Angeles Lakers have won four in a row, including a 5-1 win on Sunday against the Los Angeles Clippers.  \"I think everyone wants to believe in him. I think we can come back at the same time after this game\n",
      "1: The Los Angeles Lakers won it all by beating the Chicago Bulls 3-2 and advancing to the Finals, a game that they beat again on December 14, 1994.  They were defeated in overtime by the Los Angeles Lakers at Oracle Arena, but\n",
      "2: The Los Angeles Lakers have lost just once in their last seven games and have only been outscored by 17.6 points, a discrepancy that ranks fifth in the NBA. They've also gone 18-5 after losing eight in a row on the\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE FOR HYPERPARAMETER VARIATION 2\n",
    "inputs = tokenizer.encode(\"The Los Angeles Lakers\", return_tensors='pt').to(model.device)\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    temperature=1,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)).replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3rz6Nd2XUXc"
   },
   "source": [
    "## 2.3 Fine-Tuning GPT-2\n",
    "Okay now time for the best part!\n",
    "\n",
    "Generating general-purpose text from pre-trained models is great, but what if we want our text to be in a specific genre or style? Luckily for us, the GPT family of models use the idea of \"Transfer learning\" -- using knowledge gained from one problem (or training setting), and applying it to another area or domain. The idea of transfer learning for NLP, is that we can train a language model on general texts, and then adapt it to use it for a specific task or domain that we're interested in. This process is also called **fine-tuning**.\n",
    "\n",
    "In this section we'll walk you through an example of using HuggingFace to fine-tune GPT-2 and then you'll be asked to fine-tune GPT-2 on two datasets of your own choosing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtTdNBlOckGX"
   },
   "source": [
    "### Fine-Tuning Example using HuggingFace Datasets library: Crime and Punishment\n",
    "\n",
    "For our fine-tuning example we're going to train GPT-2 to mimic the style of Fyodor Dostoevsky's novel \"Crime and Punishment\"\n",
    "\n",
    "We will be downloading our data using the HuggingFace [Datasets](https://huggingface.co/docs/datasets/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGUNYK5M4iMa",
    "outputId": "4d950245-a87f-46a9-dd17-d10ded5b945d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nsHEIKHS2_4O"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "from datasets import load_dataset, list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyTmMnOVc_ab"
   },
   "source": [
    "### Step 1: Initialize a Brand New GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "L0JbWSYvQydJ"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').cuda()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMg4hH8nvsBn",
    "outputId": "4a689e79-45fc-469a-b1db-4e02eacbec86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}), mlm=False, mlm_probability=0.15)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVMuJpwLdGWI"
   },
   "source": [
    "###Step 2: Load the text of \"Crime and Punishment\" and tokenize it\n",
    "\n",
    "The 'load_dataset' function queries for a dataset with a certain tag and downloads the corresponding data from HuggingFace's hosting site. This allows us to download all sorts of datasets through the same interface!\n",
    "\n",
    "The documentation for load_dataset can be found [here](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset)\n",
    "\n",
    "Here we take our tokenizer and run it on the entirety of Crime and Punishment in a single batch by using map on our custom encode function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "82TWhrUBHbGs"
   },
   "outputs": [],
   "source": [
    "def encode(batch): \n",
    "    return tokenizer([x.strip('\\n\\r') for x in batch['line']], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rySikWitiWn",
    "outputId": "66b81369-6e8e-42fb-88f4-b31594b25953"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset crime_and_punish (/root/.cache/huggingface/datasets/crime_and_punish/crime-and-punish/1.0.0/87ec36ba9cb8741325bea3e40a6d4525210c8f8ef13e7b07872fe32eb72c13ac)\n"
     ]
    }
   ],
   "source": [
    "crime_and_punishment = load_dataset('crime_and_punish', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74FLAcEBt2kJ",
    "outputId": "73d1a3f2-d6e0-498c-fa42-731cf2137e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['line'],\n",
       "    num_rows: 21969\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_and_punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ3leJhxt00h",
    "outputId": "3ea7f163-fed8-42e6-ca6c-4748853b7854"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/crime_and_punish/crime-and-punish/1.0.0/87ec36ba9cb8741325bea3e40a6d4525210c8f8ef13e7b07872fe32eb72c13ac/cache-98f25f7c4eac82b8.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = crime_and_punishment.map(encode, batched=True, batch_size=len(crime_and_punishment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMkSxZsXvWmg",
    "outputId": "e0359e75-5176-4772-e3ac-b9714bba6dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'line'],\n",
       "    num_rows: 21969\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9Qsc8pJduIZ-"
   },
   "outputs": [],
   "source": [
    "processed_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq-2h_RQeYqe"
   },
   "source": [
    "### Step 3: Initialize the Trainer\n",
    "\n",
    "The 'Trainer' module is the main way we perform fine-tuning. In order to initialize a Trainer, you need a model, tokenizer, TrainingArguments, your training data (in a Dataset object) and something called a data_collator (which tells the Trainer not to look for a vector of labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3xYPXcIPADUB"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qARv2Z43vy12",
    "outputId": "549edec5-260d-4fdf-9f82-10973654e684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=/content/, overwrite_output_dir=True, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=64, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=./logs, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=100, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name=/content/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ALcp795gvwJe"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrom-2JlfvET"
   },
   "source": [
    "### Step 4: Fine-Tune the Model!\n",
    "\n",
    "Now we're done! All we have to do is hit run and sit back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "ui52Yu-pA9c3",
    "outputId": "c7bcd73e-764b-4f88-e799-f2d9a0a761b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1374' max='1374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1374/1374 03:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.503100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1374, training_loss=3.5791839199815776, metrics={'train_runtime': 223.7963, 'train_samples_per_second': 6.14, 'total_flos': 574101809809920.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 10391552, 'train_mem_gpu_alloc_delta': 1499693056, 'train_mem_cpu_peaked_delta': 11399168, 'train_mem_gpu_peaked_delta': 1278872064})"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ_D6i-Wf4eV"
   },
   "source": [
    "### Step 5: Save the Model and use it to Generate!\n",
    "\n",
    "Save your fine-tuned model and compare its output with regular GPT-2's output to see the difference for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xhRSQdbsS3fo"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('./dostoevskypt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vI8OQBmwTLF3"
   },
   "outputs": [],
   "source": [
    "dostoevskypt2 = pipeline('text-generation', model='./dostoevskypt2', device=0)\n",
    "gpt2 = pipeline('text-generation', model='gpt2', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwH-idynVTvl",
    "outputId": "dd074da4-ff48-449e-da27-419afe3b81df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Saint Petersburg is haunted by the same strange feeling of terror, of the same peculiar feeling of being alone. “When will you come in?” he asked abruptly at last. “I am sorry,” he looked more carefully,'}]\n",
      "[{'generated_text': 'Saint Petersburg is an extremely important centre – the oldest city in Russia – one of the top five in the world and one of the second-best centers of science, technology, engineering, and mathematics in the world. It boasts an amazing population of nearly'}]\n"
     ]
    }
   ],
   "source": [
    "print(dostoevskypt2('Saint Petersburg is'))\n",
    "print(gpt2('Saint Petersburg is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAtTbv1MSMoQ"
   },
   "source": [
    "## PERPLEXITY\n",
    "\n",
    "(3 pts) Using the pointer [here](https://huggingface.co/transformers/perplexity.html), compute the perplexity of the GPT2 pre-trained model on the Wikipedia test set (you can keep the same hyperparameters as in the link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4wtxSBvI0CAg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7yXtWG-UFvK",
    "outputId": "c62964c4-8cb3-4841-85a2-9fb33ff4a491"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n"
     ]
    }
   ],
   "source": [
    "test_wiki = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ha90wQwa5zz7"
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, test_data, format='text'):\n",
    "    encodings = tokenizer('\\n\\n'.join(test_data[format]), return_tensors='pt')\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    lls = []\n",
    "    for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i    # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:,begin_loc:end_loc].to(model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:,:-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    logger.info(\"perplex:{}\".format(ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSYd8sI6Mbw",
    "outputId": "774469e9-0612-4a18-d2a8-3ab60ea7daae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 562/562 [00:19<00:00, 28.50it/s]\n",
      "perplex:25.170494079589844\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON WIKIPEDIA TEST SET \n",
    "compute_perplexity(gpt2.model, gpt2.tokenizer, test_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uIEdUD-isKa"
   },
   "source": [
    "> Perplexity = 25.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbogHdnDqxEc"
   },
   "source": [
    "(2 pts) Compute the  perplexity of the dostoevskypt2 model on Wikipedia test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MbCvfbJqxEm",
    "outputId": "8e4b55ff-2809-4d71-8ae6-8097e2693e52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 562/562 [00:19<00:00, 28.48it/s]\n",
      "perplex:65.72527313232422\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF DOSTOEVSKYPT2 ON WIKIPEDIA TEST SET\n",
    "compute_perplexity(dostoevskypt2.model, dostoevskypt2.tokenizer, test_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-HXEwH28Z7V"
   },
   "source": [
    "> Perplexity = 65.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87eu1LZjTGrs"
   },
   "source": [
    "(2 pts) Compute the perplexity of the GPT2 pre-trained model on the Crime and Punishment train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHjajoo9TEyE",
    "outputId": "cd2b5ae8-279c-4e43-8614-1e277abcb240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [00:24<00:00, 28.42it/s]\n",
      "perplex:83.48881530761719\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON CRIME AND PUNISHMENT TRAIN DATASET \n",
    "compute_perplexity(gpt2.model, gpt2.tokenizer, crime_and_punishment, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5iaw2ztixEk"
   },
   "source": [
    "> Perplexity = 83.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrQR4yoPTT4K"
   },
   "source": [
    "(2 pts) Compute the **train** perplexity of the **dostoevskypt2** model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwLNNB328crd",
    "outputId": "e29e2665-583c-4422-b67c-490f0bee043f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [00:24<00:00, 28.35it/s]\n",
      "perplex:58.41886901855469\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF DOSTOEVSKYPT2 ON CRIME AND PUNISHMENT TRAIN DATASET \n",
    "compute_perplexity(dostoevskypt2.model, dostoevskypt2.tokenizer, crime_and_punishment, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajI9qA0WizXA"
   },
   "source": [
    "> Perplexity = 58.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk5r0g3wUavA"
   },
   "source": [
    "(2 pts) Compute the perplexity of the GPT2 model on your Pride and Prejudice text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7g1f1GwhBf4F"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON PRIDE AND PREJUDICE TEXT \n",
    "lines = []\n",
    "with open('prideAndPrejudice.txt') as f:\n",
    "    for line in f:\n",
    "        lines.append(line.strip())\n",
    "pride_and_prejudice={'line' : lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHphM7d6CG2N",
    "outputId": "8c32b034-5b51-4e41-d803-5a46e9bc705d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:10<00:00, 28.44it/s]\n",
      "perplex:27.930387496948242\n"
     ]
    }
   ],
   "source": [
    "compute_perplexity(gpt2.model, gpt2.tokenizer, pride_and_prejudice, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JQ--Ab9i4uf"
   },
   "source": [
    "> Perplexity = 27.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e--kB-_JUuXd"
   },
   "source": [
    "(2 pts) Compute the perplexity of the dostoevskypt2 model on your Pride and Prejudice text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RM9U2LslUuXe",
    "outputId": "715e5b7d-f3fd-4bd2-ff56-3ec9f42fb5b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:10<00:00, 28.49it/s]\n",
      "perplex:40.802947998046875\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF DOSTOEVSKYPT2 ON PRIDE AND PREJUDICE TEXT \n",
    "compute_perplexity(dostoevskypt2.model, dostoevskypt2.tokenizer, pride_and_prejudice, 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LoR4AQQi7Bl"
   },
   "source": [
    "> Perplexity = 40.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QdlBas0VHtv"
   },
   "source": [
    "1. (1 pt) Which model performs better on Wikipedia text?\n",
    "   > GPT2\n",
    "2. (1 pt) Which model performs better on Pride and Prejudice text?\n",
    "   > GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vff5qb5BSq6j"
   },
   "source": [
    "### Now's Your Turn!\n",
    "\n",
    "**Your job is to fine-tune GPT2 one more time with your choice of fine-tuning dataset.**\n",
    "\n",
    "*****For the fine-tuned model you create, you should clearly demonstrate (through visible generation outputs and analysis) that your fine-tuned model follows the desired style better than vanilla GPT2** ***\n",
    "\n",
    "Please make sure to give a brief description \n",
    "\n",
    "In order to see which datasets are available for download, run the cell below. Pick one that you think would be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kclua3zSqNz",
    "outputId": "3692ef1d-2857-4d09-d09a-4a00e0395312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(', '.join(dataset for dataset in datasets_list[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6NsclQPi4XI"
   },
   "source": [
    "### Tips\n",
    "- Most of the datasets hosted by HuggingFace are not meant for Causal LM fine-tuning. Make sure you preprocess them accordingly if you want to use them.\n",
    "- In order to check out information about a dataset hosted by huggingface you can use [this web viewer](https://huggingface.co/datasets/viewer/?dataset=crime_and_punish). Try to avoid downloading a dataset that's too big!\n",
    "- You will likely have to change the custom 'encode' function for each new dataset you want to fine-tune on. You need to change batch['line'] to instead index with the correct column label for your specific dataset (it probably wont be called 'line').\n",
    "\n",
    "### Useful Links\n",
    "[load_datasets Documentation](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset)\n",
    "\n",
    "[Trainer Documentation](https://huggingface.co/transformers/main_classes/trainer.html#id1)\n",
    "\n",
    "[Example: Fine-Tuning BERT for Esperanto](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=zTgWPa9Dipk2)\n",
    "\n",
    "[Example: Fine-Tuning for IMDb Classification](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing#scrollTo=5DEWNilys9Ty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzjWvQnft8t2"
   },
   "source": [
    "#### Dataset \\#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use ag_news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "a3M0lPs5mMkk"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE - FOR FINE-TUNING GPT2 ON DATASET \n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "from datasets import load_dataset, list_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3gQiw81-G3OV"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').cuda()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnPYjuseGyU8",
    "outputId": "7c4bd0fd-d9bd-4bea-88b0-e6c80496d0ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
     ]
    }
   ],
   "source": [
    "ag_news = load_dataset('ag_news', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXfwKL9BLEGU",
    "outputId": "04fd8ea5-7f4a-411c-997c-3a80eb737c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7600\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jpOmGu3sE2Zl"
   },
   "outputs": [],
   "source": [
    "def encode(batch): \n",
    "    return tokenizer([x.strip('\\n\\r') for x in batch['text']], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmoX7Z9DE2bs",
    "outputId": "345b7a1a-62d3-45c2-f381-ee29f1d84401"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-ac159b94f154fdc2.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = ag_news.map(encode, batched=True, batch_size=len(ag_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "2KkXy2bEE2d8"
   },
   "outputs": [],
   "source": [
    "processed_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8pAND8jHL8T",
    "outputId": "85e5db22-2a30-4898-b3dc-7400f37e905d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remove_columns_ is deprecated and will be removed in the next major version of datasets. Use the dataset.remove_columns method instead.\n"
     ]
    }
   ],
   "source": [
    "processed_dataset.remove_columns_('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksDQztm4IEiq",
    "outputId": "97a74521-207c-47a3-f3e1-cb2411992bb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'text'],\n",
       "    num_rows: 7600\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dcz1HaU0FRcK"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./log2',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Pa_tTJxxFReh"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "z7EB3wzSFRgx",
    "outputId": "76ab943d-4184-4821-fc85-4f39efa034d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 04:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.545700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=950, training_loss=3.6390905279862253, metrics={'train_runtime': 252.9898, 'train_samples_per_second': 3.755, 'total_flos': 1469683908403200.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -146526208, 'train_mem_gpu_alloc_delta': 1501162496, 'train_mem_cpu_peaked_delta': 167673856, 'train_mem_gpu_peaked_delta': 4862874112})"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6NwNFURsQe_"
   },
   "source": [
    "(3 pts) YOUR ANSWER HERE - BRIEF DESCRIPTION OF THE DATASET YOU CHOSE\n",
    "> AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fWJ6yNlysx79"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE - FOR GENERATION WITH YOUR FINE-TUNED MODEL AND COMPARISON WITH REGULAR GPT2\n",
    "from transformers import pipeline\n",
    "trainer.save_model('./ag_news_model')\n",
    "ag_news_model = pipeline('text-generation', model='./ag_news_model', device=0)\n",
    "gpt2 = pipeline('text-generation', model='gpt2', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGpVIu9XL47_",
    "outputId": "4d2b4a79-0316-4cf8-c56a-0857aeac17cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The cat is so cute, don #39;t you think? What if a great cat is so perfect that the humans don #39;t even care about them? After it was just one of those years of living in a place that could'}]\n",
      "[{'generated_text': 'The cat is so cute and cute and adorable that it\\'s so cute, even when it\\'s just for decoration, it\\'s so cute and adorable,\" said David.\\n\\nOne of the more memorable parts of the ride is seeing the cat come to'}]\n"
     ]
    }
   ],
   "source": [
    "print(ag_news_model('The cat is so cute'))\n",
    "print(gpt2('The cat is so cute'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXLqnbnhs_nC"
   },
   "source": [
    "(4 pts) YOUR ANSWER HERE - COMPARISON OF YOUR DATASET'S FINE-TUNED OUTPUT VS NON-FINE-TUNED OUTPUT \n",
    "> Due to fine-tuned on news dataset, the generated text is more like news, while the non-fine-tuned output is more general. We could generate specific sentences by using fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "VxgyTg-2yLtd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "“Homework 5-Transformers and Text-Generation (Part 1)”",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
